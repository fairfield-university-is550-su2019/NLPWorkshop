{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) with TextaCy\n",
    "\n",
    "In part 1, we focused on the preprocessing side of NLP. However, there are much more tasks with NLP. In this tutorial, we are going to cover some of these tasks, including:\n",
    "\n",
    "### Table of Contents\n",
    "- [Getting Started](#Getting-Started)\n",
    "- [Loading Data](#Loading-Data)\n",
    "- [More on Named Entities](#More-on-Named-Entities)\n",
    "- [Part-of-Speech (POS) Tagging](#Part-of-Speech-Tagging)\n",
    "- [NP & VP Chunking](#NP-&-VP-Chunking)\n",
    "- [Term Extraction](#Term-Extraction)\n",
    "- [Topic Modeling](#Topic-Modeling)\n",
    "- [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "We start with loading the `textacy` package again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "In this part, we will use the `CapitolWords()` dataset, which comes with TextaCy. We filter speeches made by Ms. Clinton and President Obama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.datasets  # note the import\n",
    "cw = textacy.datasets.CapitolWords()\n",
    "#cw.download()\n",
    "records = cw.records(limit=600)\n",
    "text_stream, metadata_stream = textacy.io.split_records(records, 'text')\n",
    "corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the basic statistics of the `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Named Entities\n",
    "\n",
    "In part 1, we touched upon named entities, however, we did not dig deep into them. Here are more on them ...\n",
    "\n",
    "Named entities are about different objects in the real world, which include: time/date (`TIME/DATE`), location (`GPE`), organization (`ORG`), people/person (`NORP/PERSON`), number (`CARDINAL`), money (`MONEY`), ...\n",
    "\n",
    "let's use the following example to show aforementioned types.\n",
    "\n",
    "__NOTE__: do you know how TextaCy knows about these entities? Answer is TextaCy relies on a pre-trained __classification__ model to \"guess\"! Since it is classification, sometimes the named entities will be mis-classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = corpus[1]\n",
    "for ent in textacy.extract.named_entities(doc, drop_determiners=True):\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP practices, named entities can be very important (for instance, to determine the context of a document, in [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval)) to not important at all (for instance, in [topic modeling](https://en.wikipedia.org/wiki/Topic_model)). So based on different tasks, we may use different strategies:\n",
    "- If they are important, extract them in a list;\n",
    "- If they are not important, replace them with their respective labels (e.g. `PERSON` for 'Clinton').\n",
    "\n",
    "### YOUR TURN HERE\n",
    "Extract all `PERSON` entities from `doc`, and store them in a list namely `person_lst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Complete your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging\n",
    "\n",
    "After tokenization, spaCy/textaCy can parse and tag a given `Doc`. This is where the statistical model comes in, which enables spaCy/textaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a __noun__.\n",
    "\n",
    "This is done using the `pos_` attribute provided with `token` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    print(t, t.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "Extract all non-stop verbs (`.pos_ == 'VERB'`) in its lemma form (`.lemma_`) from `doc`.\n",
    "\n",
    "__HINT__: non-stop words can be filtered using the `is_stop` attribute of any token `t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Complete your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Task\n",
    "\n",
    "From above results, you can observe that many verbs are duplicated in the list. How can you remove the duplicates from the list? Can you return the number of unique (non-duplicate) verbs in `doc`?\n",
    "\n",
    "__HINT__: Which of Python's data types forbids duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Complete your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP & VP Chunking\n",
    "\n",
    "From common sense, we know that words (tokens) may not be the most useful linguistic unit in text. Sometimes, phrases formed by words contain inseparable senses in text. Identifying phrases in text is called phrase chunking. Phrase chunking is a natural language process that separates and segments a sentence into its subconstituents, such as noun, verb, and prepositional phrases. [Source: Wikipedia](https://en.wikipedia.org/wiki/Phrase_chunking).\n",
    "\n",
    "In practices, we focus mainly on Noun Phrases (NP) and Verb Phrases (VP).\n",
    "\n",
    "For NP Chunking, textaCy provides a built-in method (`textacy.extract.noun_chunks()`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for np in textacy.extract.noun_chunks(doc, drop_determiners=True):\n",
    "    # this is to guarantee we are getting multi-word phrases not individual words\n",
    "    if len(np.text.split()) > 1: \n",
    "        print(np.text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For VP chunking, it is a little bit more complicated. You will use regular expression matching on textaCy's built-in VP patterns. See example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = textacy.constants.POS_REGEX_PATTERNS['en']['VP']\n",
    "for vp in textacy.extract.pos_regex_matches(doc, pattern):\n",
    "    # this is to guarantee we are getting multi-word phrases not individual words\n",
    "    if len(vp.text.split()) > 1:\n",
    "        print(vp.text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining phrase (NP & VP) chunking with named entity extraction, you can extract more complicated linguistic patterns from text data.\n",
    "\n",
    "Below code can extract multi-word named entities from `doc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in textacy.extract.named_entities(doc, drop_determiners=True):\n",
    "    # this is to guarantee we are getting multi-word phrases not individual words\n",
    "    if len(ent.text.split()) > 1:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like with the help of machine learning, machines can understand a _little bit_ of text data, right? \n",
    "\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "Next, we are going to demonstrate advanced text analytics techniques.\n",
    "\n",
    "## Term Extraction\n",
    "\n",
    "We already learned how to extract words, named entities, or phrases from text. However, in text analytics, we do not treat every word/phrases equally - some of them are more important than others. We name these 'important' words/phrases as __terms__ (short for _terminologies_). Extracting terms from texts is an important NLP task.\n",
    "\n",
    "TextaCy provides several term extraction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Keyterms for TextRank & Srank\n",
    "# make sure you import the sub-package\n",
    "import textacy.keyterms\n",
    "# SGRank\n",
    "textacy.keyterms.sgrank(doc, ngrams=(1, 2, 3, 4, 5, 6), \n",
    "                        normalize='lemma', window_width=1500, n_keyterms=10, idf=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Rank\n",
    "textacy.keyterms.singlerank(doc, normalize='lemma', n_keyterms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text rank\n",
    "textacy.keyterms.textrank(doc, normalize='lemma', n_keyterms=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these terms, can you get an understanding regarding the `doc`?\n",
    "\n",
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "## Topic Modeling\n",
    "\n",
    "The most advanced technique for document understanding is named __Topic Modeling__, which relies on the (co-)occurrences of words/tokens/terms.\n",
    "\n",
    "TextaCy provides a method (`textacy.tm.topic_model.TopicModel`) for topic modeling purposes. To creating topic modeling, we need to generate word vectors, in which each word in represented using a vector. This functionality is built on scikit-learn.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1080/1*2r1yj0zPAuaSGZeQfG6Wtw.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.vsm import Vectorizer\n",
    " \n",
    "tokenized_docs = (doc.to_terms_list(ngrams=1, entities=True, as_strings=True) \n",
    "                  for doc in corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(apply_idf=True, norm='l2', min_df=3, max_df=0.95, idf_type='smooth', \n",
    "                       tf_type='linear', max_n_terms=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.terms_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a vectorized corpus (i.e. document-term matrix) and corresponding vocabulary (i.e. mapping of term strings to column indices in the matrix), we can then initialize and train a topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textacy.tm.TopicModel('nmf', n_topics=20)\n",
    "model.fit(doc_term_matrix)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform the corpus and interpret our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then associate topics with docs in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, top_docs in model.top_topic_docs(doc_topic_matrix, topics=[0,1,2], top_n=2):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))\n",
    "    for j in top_docs:\n",
    "        print(corpus[j].metadata['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the top-10 best match docs and topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_idx, topics in model.top_doc_topics(doc_topic_matrix, docs=range(10), top_n=2):\n",
    "    print(corpus[doc_idx].metadata['title'], ':', topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also in return look at the topic loading on the whole corpus, which can be used to determine the importance of each topic (the __higher__, the __better__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in enumerate(model.topic_weights(doc_topic_matrix)):\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.termite_plot(doc_term_matrix, vectorizer.id_to_term,\n",
    "                  topics=-1,  n_terms=25, sort_terms_by='seriation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nmf-20topics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learned some advanced text analytics techniques, these techniques are either used to extract (semi-structured) information from text, or summarizing text using most important terms or topics.\n",
    "\n",
    "The techniques you learned in part 1 & 2 cover the most important NLP tasks in the field of text mining. Feel free to try them on your own. \n",
    "\n",
    "### Have fun text mining!\n",
    "\n",
    "### Useful Links\n",
    "- [TextaCy API references](https://chartbeat-labs.github.io/textacy/api_reference.html#)\n",
    "- [Natural Language Processing is Fun!](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)\n",
    "- [spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101)\n",
    "\n",
    "__PLEASE complete both parts of the tutorial and submit back using GitHub classroom.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

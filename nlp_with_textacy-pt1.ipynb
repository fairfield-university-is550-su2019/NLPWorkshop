{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING WITH TEXTACY & SPACY\n",
    "\n",
    "__Spacy__ is a very high performance NLP library for doing several tasks of NLP with ease and speed. Let us explore another library built on top of __SpaCy__ called __TextaCy__.\n",
    "\n",
    "## TEXTACY\n",
    "+ Textacy is a Python library for performing higher-level natural language processing (NLP) tasks,\n",
    "built on the high-performance Spacy library.\n",
    "+ Textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text.\n",
    "+ Uses\n",
    "    + Text preprocessing\n",
    "    + Keyword in Context\n",
    "    + Topic modeling\n",
    "    + Information Extraction\n",
    "    + Keyterm extraction,\n",
    "    + Text and Readability statistics,\n",
    "    + Emotional valence analysis,\n",
    "    + Quotation attribution\n",
    "\n",
    "### INSTALLATION\n",
    "You can install using `pip install textacy` or `conda install -c conda-forge textacy`.\n",
    "NB: In case you are having issues with installing on windows you can use conda instead of pip.\n",
    "\n",
    "### Downloading Dataset\n",
    "You can use the following command to download the `capitol_words` dataset, whcih we will use in this tutorial.\n",
    "`python -m textacy download capital_words`\n",
    "\n",
    "### FOR LANGUAGE DETECTION\n",
    "You can either use `pip install textacy[lang]` or `pip install cld2-cffi` to install the required language pack for textacy. \n",
    "\n",
    "__NOTE__: All required the package, dependencies, and add-on packs are pre-installed for this tutorial.\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, \\\n",
    "built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing \\\n",
    "— offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, \\\n",
    "and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, \\\n",
    "and more.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing — offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, and more.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT PREPROCESSING WITH TEXTACY\n",
    "Following methods can be used to preprocess your text data:\n",
    "\n",
    "+ `textacy.preprocess_text()`\n",
    "+ `textacy.preprocess.`\n",
    "    + Punctuation Lowercase\n",
    "    + Urls\n",
    "    + Phone numbers\n",
    "    + Currency\n",
    "    + Emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\" The best programs, are the ones written when the programmer is supposed to be working on something else.\\\n",
    "Mike bought the book for $50 although in Paris it will cost $30 dollars. Don’t document the problem, \\\n",
    "fix it.This is from https://twitter.com/codewisdom?lang=en. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs  are the ones written when the programmer is supposed to be working on something else Mike bought the book for $50 although in Paris it will cost $30 dollars  Don t document the problem  fix it This is from https   twitter com codewisdom lang=en  '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Punctuation and Uppercase\n",
    "processed_text = textacy.preprocess.remove_punct(raw_text)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs  are the ones written when the programmer is supposed to be working on something else Mike bought the book for $50 although in Paris it will cost $30 dollars  Don t document the problem  fix it This is from https   twitter com codewisdom lang=en  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing urls\n",
    "processed_text = textacy.preprocess.replace_urls(processed_text,replace_with='TWITTER')\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs  are the ones written when the programmer is supposed to be working on something else Mike bought the book for USD50 although in Paris it will cost USD30 dollars  Don t document the problem  fix it This is from https   twitter com codewisdom lang=en  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing Currency Symbols\n",
    "processed_text = textacy.preprocess.replace_currency_symbols(processed_text,replace_with='USD')\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we created a variable `processed_text` in every cell block above? That is because that usually text preprocessing is a pipeline - with multiple steps in it. Here are the steps we completed above:\n",
    "+ Removing Punctuation and Uppercase\n",
    "+ Removing urls\n",
    "+ Replacing Currency Symbols\n",
    "\n",
    "So we are using the variable to pass text data from each step to the next.\n",
    "\n",
    "There are much more text preprocessing steps. [Here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) is a good summary of these steps.\n",
    "\n",
    "You can incorporate all above steps together using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the best programs are the ones written when the programmer is supposed to be working on something else mike bought the book for $50 although in paris it will cost $30 dollars don t document the problem fix it this is from url'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess All\n",
    "processed_text = textacy.preprocess_text(raw_text,lowercase=True,no_punct=True,no_urls=True)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [docs](https://chartbeat-labs.github.io/textacy/api_reference.html#module-textacy.preprocess) for more details of the `textacy.preprocess` and its family methods.\n",
    "\n",
    "### READING A TEXT OR A DOCUMENT\n",
    "+ `textacy.Doc(your_text)`\n",
    "+ `textacy.io.read_text(your_text)`\n",
    "\n",
    "Textacy would not receive a lot of attractions if it only can remove URLs or punctuations; however, all additional/more advanced techniques/analyses required on `formatting` the data.\n",
    "\n",
    "TextaCy/SpaCy uses a `Doc` as a container for any text objects. [Here](https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html#make-a-doc) is nice documentation of the `doc` object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Doc\n",
    "# Requires Language Pkg Model\n",
    "docx_textacy = textacy.Doc(example, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(82 tokens; \"Textacy is a Python library for performing high...\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the `type` of the `docx_textacy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textacy.doc.Doc"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docx_textacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code read a `doc` from a local file:\n",
    "\n",
    "1. use the .read() method\n",
    "`file_textacy = textacy.Doc(open(\"example.txt\").read())`\n",
    "\n",
    "2. create a generator\n",
    "`file_textacy2 = textacy.io.read_text('example.txt',lines=True)`\n",
    "\n",
    "then:\n",
    "\n",
    "`for text in file_textacy2:`\n",
    "\n",
    "    `docx_file = textacy.Doc(text)`\n",
    "    \n",
    "    `print(docx_file)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Text Analytics \n",
    "\n",
    "1. Named-Entity Recognition\n",
    "\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. [Source: [Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)]\n",
    "\n",
    "TextaCy has a built-in method for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NLP, Spacy]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Textacy Named Entity Extraction\n",
    "list(textacy.extract.named_entities(docx_textacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. n-grams\n",
    "\n",
    "In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.\n",
    "\n",
    "TextaCy has a built-in method for n-grams.\n",
    "\n",
    "N-grams, a.k.a __Bag-of-Words__, is a very important quantifying approach for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[library for performing,\n",
       " level natural language,\n",
       " natural language processing,\n",
       " performance Spacy library,\n",
       " With the basics,\n",
       " focuses on tasks,\n",
       " availability of tokenized,\n",
       " emotional valence analysis]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NGrams with Textacy\n",
    "# NB SpaCy method would be to use noun Phrases\n",
    "# Tri Grams\n",
    "\n",
    "list(textacy.extract.ngrams(docx_textacy,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. text statistics\n",
    "This usually includes computing basic counts and various readability statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = textacy.TextStats(docx_textacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique words\n",
    "ts.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_chars': 364,\n",
       " 'n_long_words': 28,\n",
       " 'n_monosyllable_words': 29,\n",
       " 'n_polysyllable_words': 19,\n",
       " 'n_sents': 2,\n",
       " 'n_syllables': 116,\n",
       " 'n_unique_words': 51,\n",
       " 'n_words': 60}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic counts of linguistic units\n",
    "ts.basic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'automated_readability_index': 22.144,\n",
       " 'coleman_liau_index': 18.884049400000002,\n",
       " 'flesch_kincaid_grade_level': 18.923333333333336,\n",
       " 'flesch_reading_ease': 12.825000000000045,\n",
       " 'gulpease_index': 38.333333333333336,\n",
       " 'gunning_fog_index': 24.66666666666667,\n",
       " 'lix': 76.66666666666666,\n",
       " 'smog_index': 20.736966565827903,\n",
       " 'wiener_sachtextformel': 14.740666666666666}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# readability scores\n",
    "ts.readability_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these basic counts and readability stats seem intimidating. Feel free to Google them to better understand them. \n",
    "\n",
    "4. Dealing with a collection of documents (corpus)\n",
    "\n",
    "Many NLP tasks require datasets comprised of a large number of texts, which are often stored on disk in one or multiple files. textacy makes it easy to efficiently stream text and (text, metadata) pairs from disk, regardless of the format or compression of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chamber': 'Senate',\n",
       " 'congress': 107,\n",
       " 'date': '2001-02-13',\n",
       " 'speaker_name': 'Hillary Clinton',\n",
       " 'speaker_party': 'D',\n",
       " 'text': 'I yield myself 15 minutes of the time controlled by the Democrats.',\n",
       " 'title': 'MORNING BUSINESS'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textacy.datasets  # note the import\n",
    "ds = textacy.datasets.CapitolWords()\n",
    "ds.download()\n",
    "records = ds.records(speaker_name={\"Hillary Clinton\", \"Barack Obama\"})\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `textacy.Corpus` is an ordered collection of spaCy Doc s, all processed by the same language pipeline. Let’s continue with the Capitol Words dataset and make a corpus from a stream of records. (Note: This may take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(100 docs; 70500 tokens)\n"
     ]
    }
   ],
   "source": [
    "cw = textacy.datasets.CapitolWords()\n",
    "records = cw.records(limit=100)\n",
    "text_stream, metadata_stream = textacy.io.split_records(records, 'text')\n",
    "corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernie Sanders\n",
      "Lindsey Graham\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Joseph Biden\n",
      "Bernie Sanders\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "John Kasich\n",
      "John Kasich\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Joseph Biden\n",
      "Lindsey Graham\n",
      "Rick Santorum\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Rick Santorum\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "John Kasich\n",
      "John Kasich\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Rick Santorum\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "John Kasich\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Rick Santorum\n",
      "John Kasich\n",
      "Joseph Biden\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "John Kasich\n",
      "Joseph Biden\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    print(doc.metadata.get(\"speaker_name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter the corpus using certain conditions, which would cover your specific use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc(159 tokens; \"Mr. Speaker, 480,000 Federal employees are work...\")\n",
      "Doc(336 tokens; \"Mr. Speaker, I thank the gentleman for yielding...\")\n",
      "Doc(177 tokens; \"Mr. Speaker, if we want to understand why in th...\")\n"
     ]
    }
   ],
   "source": [
    "# Suppose we just want speeches of Mr. Bernie Sanders\n",
    "# Just print out top-3 for illustration\n",
    "match_func = lambda doc: doc.metadata.get(\"speaker_name\") == \"Bernie Sanders\"\n",
    "\n",
    "for doc in corpus.get(match_func, limit=3):\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus is a list-like object that can be iterated on - each element in Corpus is a `textacy.Doc` object. \n",
    "\n",
    "Which means we can do slicing as we slice any list in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(159 tokens; \"Mr. Speaker, 480,000 Federal employees are work...\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any element\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Doc(159 tokens; \"Mr. Speaker, 480,000 Federal employees are work...\"),\n",
       " Doc(219 tokens; \"Mr. Speaker, a relationship, to work and surviv...\"),\n",
       " Doc(336 tokens; \"Mr. Speaker, I thank the gentleman for yielding...\")]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sub-list\n",
    "[doc for doc in corpus[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus(15 docs; 10518 tokens)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can delete elements from `corpus`\n",
    "del corpus[:10]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get basic statistics of the `corpus` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 473, 10518)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Counts - as a dictionary\n",
    "counts = corpus.word_freqs(weighting='count', as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-PRON-', 574), ('president', 66), ('drug', 56), ('year', 52), ('mr.', 45)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get the top-5 frequent words form the `counts` dictionary\n",
    "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also introduce a new text metric called __term frequency - inversed document frequency__ (`tf-idf`). \n",
    "\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.[Source: Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Term frequency (`tf`) in `tf-idf` is the word frequency we get from above dictionary (`counts`). Now we need to calculate the `idf` part. \n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "idf(t, D) = log(\\frac{N_D}{N_t})\n",
    "\\end{equation*}\n",
    "\n",
    "In which, $ N_D $ is the number of documents (`doc`) in `corpus` D; and $ N_t $ is the number of `doc`s in $ D $ containing term $ t $.\n",
    "\n",
    "Looks complicated, right? Fortunately we can use TextaCy's built-in methods to calculate `idf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = corpus.word_doc_freqs(as_strings=True, weighting='idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10.2', 2.772588722239781),\n",
       " ('regiment', 2.772588722239781),\n",
       " ('decrease', 2.772588722239781),\n",
       " ('67', 2.772588722239781),\n",
       " ('squeeze', 2.772588722239781)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(idf.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we have both __tf__ as a dict object `counts`; and idf as a dict object `idf`, we can calculate the complete __tf-idf__ metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {k: counts[k]/idf[k] for k in idf.keys() & counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Please print out the top-20 terms with the highest `tf-idf` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-PRON-', 828.106953470265),\n",
       " ('president', 72.02954008386124),\n",
       " ('mr.', 64.92127684000336),\n",
       " ('year', 49.23996810761714),\n",
       " (\"'s\", 33.18393853462122),\n",
       " ('american', 28.817630832697375),\n",
       " ('family', 22.704800050003993),\n",
       " ('budget', 21.64042561333445),\n",
       " ('drug', 20.19773057244549),\n",
       " ('let', 19.211753888464916),\n",
       " ('country', 18.359418803402345),\n",
       " ('percent', 18.359418803402345),\n",
       " ('come', 18.33849234808015),\n",
       " ('people', 17.465230807695377),\n",
       " ('new', 15.71870772692584),\n",
       " ('government', 15.402934825195407),\n",
       " ('think', 15.15075941772835),\n",
       " ('$', 14.51087629033243),\n",
       " ('bill', 14.203836954120328),\n",
       " ('house', 13.570005202514777)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Complete your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of above results make sense, such as 'president' and 'bill' and 'act'. But terms like '-PRON-' (referring to pronouns such as 'you' or 'I') and ''s' do not make sense. Similar conclusion can be drawn onto words such as 'a', 'an', 'the', ...\n",
    "\n",
    "These words are called __stop words__. In computing, stop words are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search. [Source: Wikipedia](https://en.wikipedia.org/wiki/Stop_words).\n",
    "\n",
    "We can check if a word (a.k.a. token) is stop by using the `is_stop` attribute provided with `textacy.token` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. False\n",
      "Speaker False\n",
      ", False\n",
      "I False\n",
      "was True\n",
      "unavoidably False\n",
      "absent False\n",
      "during True\n",
      "the True\n",
      "votes False\n",
      "on True\n",
      "default False\n",
      "legislation False\n",
      ". False\n",
      "If False\n",
      "I False\n",
      "had True\n",
      "been True\n",
      "present False\n",
      ", False\n",
      "I False\n",
      "would True\n",
      "have True\n",
      "voted False\n",
      "\" False\n",
      "nay False\n",
      "\" False\n",
      "on True\n",
      "the True\n",
      "motions False\n",
      "to True\n",
      "table False\n",
      "the True\n",
      "appeal False\n",
      "of True\n",
      "the True\n",
      "ruling False\n",
      "of True\n",
      "the True\n",
      "Chair False\n",
      "with True\n",
      "regards False\n",
      "to True\n",
      "the True\n",
      "resolutions False\n",
      "offered False\n",
      "by True\n",
      "Mr. False\n",
      "Gephardt False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "26 False\n",
      ") False\n",
      "and True\n",
      "Ms. False\n",
      "Jackson False\n",
      "- False\n",
      "Lee False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "27 False\n",
      ") False\n",
      ", False\n",
      "I False\n",
      "would True\n",
      "have True\n",
      "voted False\n",
      "\" False\n",
      "nay False\n",
      "\" False\n",
      "on True\n",
      "the True\n",
      "ordering False\n",
      "of True\n",
      "the True\n",
      "previous False\n",
      "question False\n",
      "on True\n",
      "House False\n",
      "Resolution False\n",
      "355 False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "28 False\n",
      ") False\n",
      ". False\n",
      "I False\n",
      "would True\n",
      "have True\n",
      "voted False\n",
      "\" False\n",
      "nay False\n",
      "\" False\n",
      "on True\n",
      "H. False\n",
      "Con False\n",
      ". False\n",
      "Res False\n",
      ". False\n",
      "141 False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "29 False\n",
      ") False\n",
      ". False\n",
      "I False\n",
      "would True\n",
      "have True\n",
      "voted False\n",
      "\" False\n",
      "yea False\n",
      "\" False\n",
      "on True\n",
      "H.R. False\n",
      "2924 False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "30 False\n",
      ") False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "my_doc = corpus[0]\n",
    "\n",
    "for t in my_doc.tokens:\n",
    "    print(t, t.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Remove all stop words from `my_doc`.\n",
    "\n",
    "__HINT__: using an `if` statement (with the `is_stop` attribute) in the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Mr.,\n",
       " Speaker,\n",
       " ,,\n",
       " I,\n",
       " unavoidably,\n",
       " absent,\n",
       " votes,\n",
       " default,\n",
       " legislation,\n",
       " .,\n",
       " If,\n",
       " I,\n",
       " present,\n",
       " ,,\n",
       " I,\n",
       " voted,\n",
       " \",\n",
       " nay,\n",
       " \",\n",
       " motions,\n",
       " table,\n",
       " appeal,\n",
       " ruling,\n",
       " Chair,\n",
       " regards,\n",
       " resolutions,\n",
       " offered,\n",
       " Mr.,\n",
       " Gephardt,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 26,\n",
       " ),\n",
       " Ms.,\n",
       " Jackson,\n",
       " -,\n",
       " Lee,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 27,\n",
       " ),\n",
       " ,,\n",
       " I,\n",
       " voted,\n",
       " \",\n",
       " nay,\n",
       " \",\n",
       " ordering,\n",
       " previous,\n",
       " question,\n",
       " House,\n",
       " Resolution,\n",
       " 355,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 28,\n",
       " ),\n",
       " .,\n",
       " I,\n",
       " voted,\n",
       " \",\n",
       " nay,\n",
       " \",\n",
       " H.,\n",
       " Con,\n",
       " .,\n",
       " Res,\n",
       " .,\n",
       " 141,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 29,\n",
       " ),\n",
       " .,\n",
       " I,\n",
       " voted,\n",
       " \",\n",
       " yea,\n",
       " \",\n",
       " H.R.,\n",
       " 2924,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 30,\n",
       " ),\n",
       " .]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Complete your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we only care about __word tokens__, which means we need to filter out _numbers_, _punctuations_, and so forth. \n",
    "\n",
    "TextaCy provides a `is_alpha` attribute for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My True\n",
      "good True\n",
      "friend True\n",
      "from True\n",
      "Connecticut True\n",
      "raised True\n",
      "an True\n",
      "issue True\n",
      "that True\n",
      "troubles True\n",
      "me True\n",
      "about True\n",
      "this True\n",
      "proposed True\n",
      "amendment True\n",
      "that True\n",
      "the True\n",
      "distinguished True\n",
      "Senator True\n",
      "from True\n",
      "Utah True\n",
      "has True\n",
      "put True\n",
      "forth True\n",
      ". False\n",
      "\n",
      " False\n",
      "In True\n",
      "addition True\n",
      "to True\n",
      "the True\n",
      "issues True\n",
      "that True\n",
      "Senator True\n",
      "Kennedy True\n",
      "and True\n",
      "Senator True\n",
      "Dodd True\n",
      "have True\n",
      "raised True\n",
      "about True\n",
      "the True\n",
      "vagueness True\n",
      "and True\n",
      "definitional True\n",
      "concerns True\n",
      "raised True\n",
      "in True\n",
      "the True\n",
      "amendment True\n",
      ", False\n",
      "this True\n",
      "particular True\n",
      "issue True\n",
      "is True\n",
      "the True\n",
      "real True\n",
      "heart True\n",
      "of True\n",
      "the True\n",
      "parity True\n",
      "problem True\n",
      "that True\n",
      "many True\n",
      "of True\n",
      "us True\n",
      "have True\n",
      "with True\n",
      "this True\n",
      "amendment True\n",
      ". False\n",
      "\n",
      " False\n",
      "It True\n",
      "reminds True\n",
      "me True\n",
      "of True\n",
      "the True\n",
      "old True\n",
      "Anatole True\n",
      "France True\n",
      "saying True\n",
      ": False\n",
      "The True\n",
      "law True\n",
      "is True\n",
      "fair True\n",
      "; False\n",
      "neither True\n",
      "the True\n",
      "rich True\n",
      "nor True\n",
      "the True\n",
      "poor True\n",
      "can True\n",
      "sleep True\n",
      "under True\n",
      "the True\n",
      "bridge True\n",
      ". False\n",
      "What True\n",
      "we True\n",
      "have True\n",
      "is True\n",
      "an True\n",
      "amendment True\n",
      "that True\n",
      "in True\n",
      "its True\n",
      "practice True\n",
      "not True\n",
      "only True\n",
      "would True\n",
      "fall True\n",
      "disproportionately True\n",
      "on True\n",
      "unions True\n",
      "as True\n",
      "compared True\n",
      "to True\n",
      "corporations True\n",
      "but True\n",
      "which True\n",
      ", False\n",
      "under True\n",
      "the True\n",
      "rationale True\n",
      "put True\n",
      "forward True\n",
      "by True\n",
      "it True\n",
      ", False\n",
      "completely True\n",
      "leaves True\n",
      "out True\n",
      "other True\n",
      "membership True\n",
      "groups True\n",
      ", False\n",
      "as True\n",
      "the True\n",
      "Senator True\n",
      "from True\n",
      "Connecticut True\n",
      "so True\n",
      "rightly True\n",
      "points True\n",
      "out True\n",
      ". False\n",
      "\n",
      " False\n",
      "The True\n",
      "burdensome True\n",
      "reporting True\n",
      "requirements True\n",
      "that True\n",
      "are True\n",
      "imposed True\n",
      "under True\n",
      "this True\n",
      "amendment True\n",
      "on True\n",
      "unions True\n",
      "in True\n",
      "particular True\n",
      "are True\n",
      "really True\n",
      "much True\n",
      "more True\n",
      "difficult True\n",
      "to True\n",
      "comply True\n",
      "with True\n",
      "than True\n",
      "if True\n",
      "they True\n",
      "would True\n",
      "be True\n",
      "in True\n",
      "a True\n",
      "corporation True\n",
      ". False\n",
      "As True\n",
      "I True\n",
      "understand True\n",
      "the True\n",
      "amendment True\n",
      ", False\n",
      "corporations True\n",
      "would True\n",
      "be True\n",
      "required True\n",
      "to True\n",
      "report True\n",
      "only True\n",
      "on True\n",
      "expenditures True\n",
      "from True\n",
      "their True\n",
      "own True\n",
      "general True\n",
      "treasuries True\n",
      "and True\n",
      "from True\n",
      "the True\n",
      "general True\n",
      "treasuries True\n",
      "of True\n",
      "their True\n",
      "subsidiaries True\n",
      ". False\n",
      "However True\n",
      ", False\n",
      "unions True\n",
      "would True\n",
      "be True\n",
      "required True\n",
      "to True\n",
      "report True\n",
      "on True\n",
      "the True\n",
      "expenditures True\n",
      "from True\n",
      "all True\n",
      "of True\n",
      "their True\n",
      "affiliates True\n",
      ", False\n",
      "which True\n",
      "would True\n",
      "mean True\n",
      "that True\n",
      "a True\n",
      "local True\n",
      "union True\n",
      "would True\n",
      "be True\n",
      "required True\n",
      "to True\n",
      "report True\n",
      "on True\n",
      "expenditures True\n",
      "by True\n",
      "a True\n",
      "national True\n",
      "union True\n",
      ", False\n",
      "and True\n",
      "vice True\n",
      "versa True\n",
      ", False\n",
      "even True\n",
      "though True\n",
      "neither True\n",
      "of True\n",
      "them True\n",
      "had True\n",
      "either True\n",
      "access True\n",
      "or True\n",
      "control True\n",
      "to True\n",
      "the True\n",
      "financial True\n",
      "records True\n",
      "of True\n",
      "the True\n",
      "other True\n",
      ". False\n",
      "\n",
      " False\n",
      "This True\n",
      "point True\n",
      "we True\n",
      "heard True\n",
      "about True\n",
      "from True\n",
      "Senator True\n",
      "Dodd True\n",
      "is True\n",
      "particularly True\n",
      "important True\n",
      ". False\n",
      "If True\n",
      "the True\n",
      "point True\n",
      "we True\n",
      "are True\n",
      "trying True\n",
      "to True\n",
      "get True\n",
      "at True\n",
      "with True\n",
      "this True\n",
      "amendment True\n",
      "is True\n",
      "to True\n",
      "understand True\n",
      "who True\n",
      "is True\n",
      "doing True\n",
      "what True\n",
      "with True\n",
      "what True\n",
      "funds True\n",
      "to True\n",
      "engage True\n",
      "in True\n",
      "political True\n",
      "activity True\n",
      "during True\n",
      "election True\n",
      "cycles True\n",
      ", False\n",
      "then True\n",
      "clearly True\n",
      "a True\n",
      "lot True\n",
      "of True\n",
      "the True\n",
      "other True\n",
      "membership True\n",
      "groups True\n",
      "that True\n",
      "raise True\n",
      "and True\n",
      "spend True\n",
      "tremendous True\n",
      "amounts True\n",
      "of True\n",
      "money True\n",
      "— False\n",
      "two True\n",
      "were True\n",
      "mentioned True\n",
      ", False\n",
      "the True\n",
      "NRA True\n",
      ", False\n",
      "the True\n",
      "Sierra True\n",
      "Club True\n",
      ", False\n",
      "you True\n",
      "can True\n",
      "add True\n",
      "the True\n",
      "Chambers True\n",
      "of True\n",
      "Commerce True\n",
      ", False\n",
      "National True\n",
      "Right True\n",
      "to True\n",
      "Work True\n",
      "Foundation True\n",
      ", False\n",
      "other True\n",
      "groups True\n",
      "across True\n",
      "the True\n",
      "political True\n",
      "spectrum---- False\n"
     ]
    }
   ],
   "source": [
    "for t in my_doc.tokens:\n",
    "    print(t, t.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "What if we want non-stop and word tokens?\n",
    "\n",
    "__HINT__: combine the two above steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Speaker,\n",
       " I,\n",
       " unavoidably,\n",
       " absent,\n",
       " votes,\n",
       " default,\n",
       " legislation,\n",
       " If,\n",
       " I,\n",
       " present,\n",
       " I,\n",
       " voted,\n",
       " nay,\n",
       " motions,\n",
       " table,\n",
       " appeal,\n",
       " ruling,\n",
       " Chair,\n",
       " regards,\n",
       " resolutions,\n",
       " offered,\n",
       " Gephardt,\n",
       " rollcall,\n",
       " No,\n",
       " Jackson,\n",
       " Lee,\n",
       " rollcall,\n",
       " No,\n",
       " I,\n",
       " voted,\n",
       " nay,\n",
       " ordering,\n",
       " previous,\n",
       " question,\n",
       " House,\n",
       " Resolution,\n",
       " rollcall,\n",
       " No,\n",
       " I,\n",
       " voted,\n",
       " nay,\n",
       " Con,\n",
       " Res,\n",
       " rollcall,\n",
       " No,\n",
       " I,\n",
       " voted,\n",
       " yea,\n",
       " rollcall,\n",
       " No]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Complete you code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, have you noticed that different forms of the same token may appear in the text? For instance, 'run', 'ran', and 'running' are all different forms of the root word 'run'. Counting them as different words may bias any subsequent model. Thus, it would be ideal to make different forms of the same word to the root. This process is called __lemmatization__.\n",
    "\n",
    "_Lemmatization_ usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the _lemma_. If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source. [Source: Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "TextaCy provides a `lemma_` attribute for `token` object for exactly this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. mr.\n",
      "Speaker speaker\n",
      ", ,\n",
      "I -PRON-\n",
      "was be\n",
      "unavoidably unavoidably\n",
      "absent absent\n",
      "during during\n",
      "the the\n",
      "votes vote\n",
      "on on\n",
      "default default\n",
      "legislation legislation\n",
      ". .\n",
      "If if\n",
      "I -PRON-\n",
      "had have\n",
      "been be\n",
      "present present\n",
      ", ,\n",
      "I -PRON-\n",
      "would would\n",
      "have have\n",
      "voted vote\n",
      "\" \"\n",
      "nay nay\n",
      "\" \"\n",
      "on on\n",
      "the the\n",
      "motions motion\n",
      "to to\n",
      "table table\n",
      "the the\n",
      "appeal appeal\n",
      "of of\n",
      "the the\n",
      "ruling ruling\n",
      "of of\n",
      "the the\n",
      "Chair chair\n",
      "with with\n",
      "regards regard\n",
      "to to\n",
      "the the\n",
      "resolutions resolution\n",
      "offered offer\n",
      "by by\n",
      "Mr. mr.\n",
      "Gephardt gephardt\n",
      "( (\n",
      "rollcall rollcall\n",
      "No no\n",
      ". .\n",
      "26 26\n",
      ") )\n",
      "and and\n",
      "Ms. ms.\n",
      "Jackson jackson\n",
      "- -\n",
      "Lee lee\n",
      "( (\n",
      "rollcall rollcall\n",
      "No no\n",
      ". .\n",
      "27 27\n",
      ") )\n",
      ", ,\n",
      "I -PRON-\n",
      "would would\n",
      "have have\n",
      "voted vote\n",
      "\" \"\n",
      "nay nay\n",
      "\" \"\n",
      "on on\n",
      "the the\n",
      "ordering ordering\n",
      "of of\n",
      "the the\n",
      "previous previous\n",
      "question question\n",
      "on on\n",
      "House house\n",
      "Resolution resolution\n",
      "355 355\n",
      "( (\n",
      "rollcall rollcall\n",
      "No no\n",
      ". .\n",
      "28 28\n",
      ") )\n",
      ". .\n",
      "I -PRON-\n",
      "would would\n",
      "have have\n",
      "voted vote\n",
      "\" \"\n",
      "nay nay\n",
      "\" \"\n",
      "on on\n",
      "H. h.\n",
      "Con con\n",
      ". .\n",
      "Res res\n",
      ". .\n",
      "141 141\n",
      "( (\n",
      "rollcall rollcall\n",
      "No no\n",
      ". .\n",
      "29 29\n",
      ") )\n",
      ". .\n",
      "I -PRON-\n",
      "would would\n",
      "have have\n",
      "voted vote\n",
      "\" \"\n",
      "yea yea\n",
      "\" \"\n",
      "on on\n",
      "H.R. h.r.\n",
      "2924 2924\n",
      "( (\n",
      "rollcall rollcall\n",
      "No no\n",
      ". .\n",
      "30 30\n",
      ") )\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for t in my_doc.tokens:\n",
    "    print(t, t.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise, you are going to complete following tasks.\n",
    "1. From the `corpus` variable, generate a new list named `jb_lst` that contains all speeches from `Joseph Biden`. (__HINT__: use similar code as we filter `cw` for 'Bernie Sanders'.)\n",
    "2. Select first 5 speeches (`doc`) from `jb_lst` and store them in a new list named `jb_selected`.\n",
    "3. For each element in `jb_selected`, print out:\n",
    "    a. Named Entities (`named_entities()`)\n",
    "    b. Text Statistics (`TextStats()`)\n",
    "4. For each element in `jb_selected`, print out the top 20 words based on their tf-idf score.\n",
    "5. For each element in `jb_selected`, print out __lemmas__ (`lemma_`) for each token if it is not a stop word (`is_stop`) and is a word token (`is_alpha`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
